*
* 													CLASS 01_Batch
* 
* 
* 	In a situation where we generate reports, we might want to use messages in batch. We've starting this
* 	class discussing the subject and creating a new service, "service-reading-report", to generate this 
*	kind of report.
*	
*	Then, we will create a new service, that will be responsible for creating a reports in batch (imagine 
*	it as a service that will run monthly and generate reports for every single user we have in this DB).
*	
*	1. About the GenerateAllReportsServlet file
*	
*		This servlet was created to trigger the report generation for many users in a single row. With that being said,
*		let's think: our servelet will receive an HTTP request that shall trigger the generation of reports, but, as we've
*		discussed in the Course 02, I want to send that data as fast as I can to the brokers, so I don't fall into the 
*		problem of something goes wrong and I loose those data inside my servlet, or, even worst, I generate messages for
*		part of the users I've received, and not all of them (them, in the frontend, my user may use F5 in the page, and
*		the full process might be triggered again).
*
*		So, I must do something like a Fast Delegate, and give that messages to someone as fast as I can. How? 
*		
*	
*			The way we are going to do it is: I receive the request in the servlet, send ONE single message to the brokers 
*			telling them to "generate all reports" and then send, as fast as I possibly can, an answer to the frontend 
*			with an OK status;  
*		
*			  	The problem: in this new service, I won't have the user's ids I've received with the HTTP request
*							 on the GenerateAllReportsServlet. So, how to deal with this? There are some ways...
*		
*					1. In the new service, I make an HTTP request to OTHER service, asking the users Ids... and await the		
*					   answer before proceed (since the user won't be awaiting the answer no more... this could be an OK
*					   approach).
*		
*					2. In the new service, I could access the database from the CreateUsers's service to find all the ids
*					   (not that good because I make two services depends on a single database... but valid, depending on
*					   the situation);
*
*					3. In the new service, I can storage a list of users ids. To do so, we could send a message to the broker
*					   each time a user is registered in my database inside the service-create-user, and this new service could
*					   be a consumer to that messages, saving locally each of those user's data so it always has a list of users's
*					   ids to generate the full report (it may generate some other problems, as generate messages when the user
*					   is deleted, so my list of user ids may always be valid).
*
*					4. From the GenerateAllReportsServlet, I can execute a "task dispatcher", that will send a message with a task
*					   to be executed from all users. This will be the approach we shall execute in this class.
*
*					   	So, to do this we created the BatchSendMessageService, a Kafka's consumer that will listen to messages of 
*						the type SEND_MESSAGE_TO_ALL_USERS and send any topics that may come with this message to all existing users
*						in our database. 
*						
*						Since we are using it to send a USER_GENERATE_READING_REPORT for all users, it will send it a message to this
*						topic in our broker, and the consumer we've already created in the start of this class, the ReadingReportService,
*						will listen to it and generate a report for each of the users it receive.
*
*
*
* 													CLASS 02_Customized Serialization and Deserialization
*
*	1. The importance of the CorrelationId
*
*		There are still errors happening here and there. Think about it: we have an HTTP Server that will receive a request and send
*		a message, that the broker will hear and a consumer will consume; the consumer will then send ANOTHER message to the broker,
*		that yet ANOTHER consumer will hear, and only then that last one will generate the reports as we want. So... how can we track
*		that process, so we may know which message comes from where? We need to know that, because an exception may occur in any part
*		of this process (or even another one, that may be larger or have even more services in between), and we need to have a way to
*		track what started the process where the exception happened.
*
*		To do that, we need a identification for the messages that we are generating, and that id must be created in the first moment
*		the process started, and keep being pass forward through each of the services that will be triggered by that first one.
*
*		That's what we are going to do in this class.
*
*
*	2. Setting up a CorrelationId
*
*		Since the correlationId will be something that every message must have, we can implement it in some ways:
*
*			1. Using the header's system that Kafka give us and put it in there;
*			2. Creating a class to wrap the message and the correlationId in a single place;
*
*		Here, we are going to do the second option.
*
*
*	3. Implementing the Correlation Id
*	
*		So, to really put the correlationId concept to use, we need to create it when we dispatch messages. But not only that; with the
*		id, we want to send a text telling where the process started, so we have a breadcrumb from where to start looking if something
*		goes wrong. 
*
*		So, the first change we are going to make is change our KafkaDispatcher to add this identifier in it.
*	
*		Now that we are adding the service that dispatch the message, we want to concatenate informations that comes from other service
*		to the current before dispatch again. What do I mean? Imagine we dispatch a message in the HttpClient. Then, our BatchSendMessage
*		will hear it ang generate a new dispatch, that will trigger the report generation for each user. In this case, we will concatenate
*		the info that came with the HTTP service with the info from the Batch service before dispatch it again, and keep doing it for any
*		services that comes in our way.

*		THis way, our correlationId will become a breadcrumb... and we might be able to use it at our favor at any time.
*
*		To do this, the first step will be turn the creation of the correlationId something required to each sender, so the people who use
*		the dispatchers will always know that info NEEDS to be there, so we may create our breadcrumb.
*
*		Then, since the services will be responsible for instantiating the CorrelationId, we can use it to our advantage to associate the
*		existing one, that our server might have received in it's method, with the new one, that it must send with the message that goes 
*		to the broker
*
* 		If we have any async requests in the middle of the service-calling-service road, we might want to add it in the CorrelationId too,
*		so we can have the best breadcrumb ever! :D
*
*
*	4. Architecture and failures until here
*
*		Let's review some specific aspects of Kafka and our application until here. We start running all our consumers, and then going to 
*		the commandline to execute some kafka's command.
*
*		Execute two commands in two different tabs:
*
*												.\kafka-topics.bat --describe --bootstrap-server localhost:9092
*
*									.\kafka-consumer-groups.bat --all-groups --bootstrap-server localhost:9091 --describe
*	
*
*		Let's analyze the info they give one by one. First, Kafka-topics:						
*
* Topic: ECOMMERCE_USER_GENERATE_READING_REPORT   PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
*         Topic: ECOMMERCE_USER_GENERATE_READING_REPORT   Partition: 0    Leader: 1       Replicas: 1,3,4 Isr: 4,1,3
*         Topic: ECOMMERCE_USER_GENERATE_READING_REPORT   Partition: 1    Leader: 2       Replicas: 2,4,1 Isr: 4,1,2
*         Topic: ECOMMERCE_USER_GENERATE_READING_REPORT   Partition: 2    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2
*         
* Topic: ECOMMERCE_SEND_EMAIL     PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
*         Topic: ECOMMERCE_SEND_EMAIL     Partition: 0    Leader: 4       Replicas: 4,1,2 Isr: 4,2,1
*         Topic: ECOMMERCE_SEND_EMAIL     Partition: 1    Leader: 1       Replicas: 1,2,3 Isr: 3,1,2
*         Topic: ECOMMERCE_SEND_EMAIL     Partition: 2    Leader: 2       Replicas: 2,3,4 Isr: 4,3,2
*
*       											...
*
* Topic: __consumer_offsets       PartitionCount: 50      ReplicationFactor: 3    Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600
*         Topic: __consumer_offsets       Partition: 0    Leader: 4       Replicas: 4,1,2 Isr: 4,2,1
*         Topic: __consumer_offsets       Partition: 1    Leader: 1       Replicas: 1,2,3 Isr: 3,1,2
*         Topic: __consumer_offsets       Partition: 2    Leader: 2       Replicas: 2,3,4 Isr: 4,3,2
*         Topic: __consumer_offsets       Partition: 3    Leader: 3       Replicas: 3,4,1 Isr: 4,1,3
*
*
*       											...
*
*		They show us information about all the topics that are being used. Partition shows in which partition the topic is being executed,
*		the ReplicationFactor shows in how many brokers that topic is being used (those brokers are the backup, since any of then  might 
*		fall, and others might assume the job).
*
*			Leader: 	the broker who that is executing the messages as current;
*			Replicas:	the brokers that might assume the job if the Leader falls for any reason;
*			Isr:		where the information about the topics (the messages they received) are up to date;
*
*		When we use ACKS, we might have confirmations if the messages are used replicated in replicas, if the message was recorded only in
*		the Leaders, etc. 
*
*		The "__consumer_offsets" shows some information about the offsets (messages generated) of each consumer in relation to each of the topics. 
*
*
*		Now, let's talk about the info we received with kafka-consumer-groups.
*
* GROUP           							  	TOPIC                					PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG    	CONSUMER-ID                                                               		HOST            CLIENT-ID
* EmailService    							  	ECOMMERCE_SEND_EMAIL 					0          0               0               0      	bd02c8eb-d3d8-45df-81c3-a09245852c53-c166c8cb-bf3f-49ce-ad94-39721ee094e3 /192.168.0.11   bd02c8eb-d3d8-45df-81c3-a09245852c53
* EmailService    							  	ECOMMERCE_SEND_EMAIL 					1          1               1               0      	bd02c8eb-d3d8-45df-81c3-a09245852c53-c166c8cb-bf3f-49ce-ad94-39721ee094e3 /192.168.0.11   bd02c8eb-d3d8-45df-81c3-a09245852c53
* EmailService    							  	ECOMMERCE_SEND_EMAIL 					2          1               1               0      	bd02c8eb-d3d8-45df-81c3-a09245852c53-c166c8cb-bf3f-49ce-ad94-39721ee094e3 /192.168.0.11   bd02c8eb-d3d8-45df-81c3-a09245852c53
* 											  											
* GROUP           							  	TOPIC                                  	PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG    	CONSUMER-ID		               													HOST            CLIENT-ID
* 											  												
* LogService      							  	ECOMMERCE_USER_GENERATE_READING_REPORT 	2          0               0               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_SEND_EMAIL                   	0          0               0               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_NEW_ORDER                    	0          0               0               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS    	2          0               0               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_USER_GENERATE_READING_REPORT 	1          0               0               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* 											  												
* LogService      							  	ECOMMERCE_SEND_EMAIL                   	1          1               1               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_USER_GENERATE_READING_REPORT 	0          4               4               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_SEND_EMAIL                   	2          1               1               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_NEW_ORDER                    	1          1               1               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_NEW_ORDER                    	2          1               1               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS    	0          0               0               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* LogService      							  	ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS    	1          6               6               0      	00588266-e4e0-458f-a39f-de183bcf9870-44064dfe-33de-464e-8655-19a00a773c8b /192.168.0.11   00588266-e4e0-458f-a39f-de183bcf9870
* 
* GROUP                                         TOPIC                                 	PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG    	CONSUMER-ID                                										HOST            CLIENT-ID
* 																						
* curso_kafka_ecommerce.BatchSendMessageService ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS   	0          0               0               0      	db248dc6-fd90-4ec7-b286-41ea67a51a24-910cc981-c2df-4330-93d0-3da3f31ea913 /192.168.0.11   db248dc6-fd90-4ec7-b286-41ea67a51a24
* curso_kafka_ecommerce.BatchSendMessageService ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS   	1          6               6               0      	db248dc6-fd90-4ec7-b286-41ea67a51a24-910cc981-c2df-4330-93d0-3da3f31ea913 /192.168.0.11   db248dc6-fd90-4ec7-b286-41ea67a51a24
* curso_kafka_ecommerce.BatchSendMessageService ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS   	2          0               0               0      	db248dc6-fd90-4ec7-b286-41ea67a51a24-910cc981-c2df-4330-93d0-3da3f31ea913 /192.168.0.11   db248dc6-fd90-4ec7-b286-41ea67a51a24
* curso_kafka_ecommerce.BatchSendMessageService SEND_MESSAGE_TO_ALL_USERS             	2          0               0               0      	-
*                                           -               -						    	
* curso_kafka_ecommerce.BatchSendMessageService SEND_MESSAGE_TO_ALL_USERS             	1          0               0               0      	-
*                                           -               -						    	
* curso_kafka_ecommerce.BatchSendMessageService SEND_MESSAGE_TO_ALL_USERS             	0          11              11              0      	-
*                                           -               -                         	
* 																						
* Consumer group 'curso_kafka_ecommerce.CreateUserService' has no active members.     	
* 																						
* GROUP                                   	   	TOPIC               					PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID     																HOST            CLIENT-ID
* curso_kafka_ecommerce.CreateUserService 	   	ECOMMERCE_NEW_ORDER 					2          1               1               0          -               																-               -
* curso_kafka_ecommerce.CreateUserService 	   	ECOMMERCE_NEW_ORDER 					1          1               1               0          -               																-               -
* curso_kafka_ecommerce.CreateUserService 	   	ECOMMERCE_NEW_ORDER 					0          0               0               0          -               																-               -
* 																
* GROUP                                      	TOPIC               					PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID	                       											HOST            CLIENT-ID
* 											                                                                                                  
* curso_kafka_ecommerce.FraudDetectorService 	ECOMMERCE_NEW_ORDER 					0          0               0               0          c1acbef7-9208-4289-a093-4b3303bacbe5-119b95e8-60b0-4a43-bc1c-4daedbeeaee1 /192.168.0.11   c1acbef7-9208-4289-a093-4b3303bacbe5
* curso_kafka_ecommerce.FraudDetectorService 	ECOMMERCE_NEW_ORDER 					1          1               1               0          c1acbef7-9208-4289-a093-4b3303bacbe5-119b95e8-60b0-4a43-bc1c-4daedbeeaee1 /192.168.0.11   c1acbef7-9208-4289-a093-4b3303bacbe5
*
* 																											...
*	
*		The consumer groups shows all those groups. Remember: for each service, we created a different group, so you can see, for instance, that the 
*		BatchSendMessageService is listening to the topic ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS in the Partitions 0, 1 and 2. The Client-Id represents
*		the consumer identification.
*
*		Note also that the BatchSendMessage does not have any info for the topic SEND_MESSAGE_TO_ALL_USERS. That's because that consumer USED to listen
*		to that topic, but we've stopped it and changed the topic... so the info shows as you see.
*
*
*			Current-offset: shows the number of messages that the broker read for that partition;
*
*			Log-end-offset: shows the number of messages that are to read in a giving partition (the total amount);
*		
*		With the info above, we can see if we all the messages where processed or something still missing. Cool, right?
*	
*		Now, let's talk about the consumer properties.
*	
*		When we create the KafkaService (our wrapper to a KafkaConsumer), we use some properties. 
*
*		A. The .poll() and rebalance
*			
*			The poll() property, basically, tells to the server "Hey, I'm alive in here. Got something for me?" and wait for the answer
*			in the specified interval.
*
*			So, it have a part to play in the the rebalance. When another instance of a specific consumer gets up, it warns Kafka that it's alive,
*			and Kafka, in his turn, rebalances the full application so it can use the new consumer in a given partition.
*
*			The rebalance will only happen when the server is communication with the consumers. A consumer that already exists doesn't know another
*			one is up, so the new one always have to tell the server that it got up. So a frequent .poll() helps to tell the server faster that a 
*			new instance is up.
*
*			The poll returns a number of records from Kafka, and we can define the max number it will returns. The higher the number of messages 
*			we're going to return from the poll, the higher the chance something goes wrong while those messages are processed and something goes
*			wrong without we knowing.
*
*			So we usually use a default configuration of "records to be consumed before the poll() returns" as 1.
*
*		B. Heartbeat and rebalance	
*
*			There's another concept called heartbeat, where a consumer tells the consumer its alive too. When the hearthbeat stops, the server
*			understands that the consumer is down and then trigger a rebalance of partitions.
*
*
*	
*
*