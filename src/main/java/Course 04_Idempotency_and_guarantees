*
* 												CLASS 01_Organization and e-mail services
* 
*	A. Reorganizing our project
*
*	 	Let's start this class refactoring our common-kafka, separating all the classes that works with the consumer and 
*		the ones that work with dispatchers in different packages.
* 
*		Then, let's talk about the e-mail service. Do we really need a service for that? Couldn't be just a library? Course
*		it could. The difficult would be that the e-mail is an external system, and the way we'll communicate with it is 
*		important.
*
*
*	B. Discussing the e-mail service
*
*		Imagine that we've used the e-mail as a common library, as we're using the common-kafka. Now, I want to change the
*		way the e-mail is generated from using Azure to AWS. If the e-mail is just a library, every service that implements
*		it must migrate at same time BEFORE I can start use the new way, which creates a dependency between all the services
*		and the library.
*
*		That's a problem, a really bad one. Using a service, we can avoid that kind of problem, making the evolution of the
*		email something detached from the other services.
*
*		Still, there's something to consider: the way we're working right now, the e-mail text will be generated inside each
*		service that sends the message to the dispatcher. What if we wanted something greater? Or, thinking in another way:
*		what if we want to send an attachment in our e-mail? How could we deal with specific situations using something as
*		generic as the e-mail service?
*
*			Answer: we could create other, more specific services, to work as middle man in those situations and make the
*					specific configurations happen for us. Awesome, huh? The magic of microservices coming to life with
*					kafka.
*
*					To show this approach, we're going to create a little, tiny bit of program called service-email-new-order,
*					which will be responsible to create an e-mail that our service-new-order is currently creating by dispatching
*					the event in it's main.
*
*						Another advantage of this approach is that we're going to apply the Fast Delegate to the e-mail generation,
*						which is incredible!!!
*
*
* 														CLASS 02_The service layer
*
*
*	A. Extracting the KafkaService generation to a layer of services
*
*		Currently, if we want to run two instances at the same time, we need to run it manually. Each of the instances will be
*		run in it's own process. But there are situations where we might want to execute 10 e-mail services inside the same
*		process on own JVM (in other words, execute the instances in different threads).
*
*			This would make the startup's loading higher
*
*		So, in this class we going to start preparing the email service for this  First, but not least, we need to see if the
*		email has any state that it shares with it's methods. The problem is: it uses the KafkaService, that has a internal
*		state.
*
*		So, we need to see in the Kafka's docs if the Consumer and the Producer are thread safe. We can see that the producer
*		is... but the consume isn't. In its documentation, Kafka tells us how to deal with this situation and execute a consumer
*		in different threads. 
*
*			Well.. how are we going to deal with this?
*
*				In this project, we are not going to share the Kafka consumer in multi-threads... instead, we are going to create
*				one instance of service-email per thread (so each thread will got it's own consumer).
*
*
*		We are going to start by refactoring our email service, allowing it to be used as a service like it should be. You can see
*		in this comment how we are going to do it.
*
*
*	B. Parallelizing with threads's pools
*
*		Now that we've refactored our service-email, we can start to run it in multiple threads.
*
*		We have now the serviceProvider() function. We want it to be called 10 times. One approach would be make it implement the runnable
*		interface. But we are not going to do this. The runnable does not allows to throw an exception inside of it... but there is another
*		one who does.
*
*		We want to call that function N times, so we make it implements a callable of type void (because it'll never return anything).
*
*		When you do this, you'll have N instances consuming in a service group, so its good to make sure that each one of then has it's 
*		own CLIENT_ID_CONFIG, a config we can add on the consumer properties (in our case, they already have).
*
*		But what if an exception is thrown when trying to generate a deadletter?
*
*			Then, the thread will die, but the others will keep running (actually, since we're creating with newFixedThreadPool, the library
*			will restart the thread that falls... BUT, our consumer won't be submitted again to that thread. In other words: the thread will
*			be alive, our consumer won't).
*
*		Since we are here, let's go add the same "thread support" to the ReadingReportService. Once we've done it, we can run it and go to
*		the cmdline and see what are the consumer groups being executed. You'll see something like this, if you've been following this course
*		from the beginning:
*
* 			GROUP                                  					TOPIC                     PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  	LAG      									CONSUMER-ID 									HOST            		CLIENT-ID	
* 			                                          
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  1          0               0               0        7c1d3b0f-8d77-40af-8ab2-4f1aba6afd99-92d167c2-7fe9-4c9e-b190-9689d5440985 	/192.168.0.11   7c1d3b0f-8d77-40af-8ab2-4f1aba6afd99
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  2          0               0               0        830d4708-a65f-4da1-a090-df5989ec46cc-7190aacf-0192-49b8-9286-aff774f39055 	/192.168.0.11   830d4708-a65f-4da1-a090-df5989ec46cc
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  0          0               0               0        6fb31ff7-f7b9-444f-8793-012f323b7f63-d1145e5f-28dc-4bdf-9bfe-c8113d92a024 	/192.168.0.11   6fb31ff7-f7b9-444f-8793-012f323b7f63
* 			
* 			Consumer group 'EmailService' has no active members.
*
*
*		Note how there are three consumers being executed for the topic, the one we've changed. That happens because we've configure our kafka's 
* 		broker to use 3 partitions when executed... and, inside our code, we've chosen to run 5 threads with instances of the ReadingReportService.
*
*			In other words: there are 5 ReadingReportService instances running in 5 different threads, but just 3 of then are in kafka's 
*			partitions since we've enabled the broker to execute three partitions at time (remember, that was defined in the server.properties
*			file, that stays in config folder).
*
*
* 														CLASS 03_Commits and offsets
*
*
*	A. Offset latest and earliest
*
*		Remember: by default, our partitions use 3 replicas. Let's start this class erasing the data directory for each server and zookeper, and
*		restarting everything once more. Then, let's update our service-new-order to use the thread way we've created in last class, and put it 
*		to use a single thread.
*
*		Once done it, let's the HTTP service WITHOUT the service-new-email and see what happens. When we execute the /new of the http service, kafka 
*		will create a topic (the one called by the servlet), but no consumer group is created, since we've not started our consumer yet. Now, we run 
*		our service-new-order.
*
*		If you read the service-new-order logs, you'll discover it reset the partition offset. Why? When we start a new consumer group, it can begin
*		to listen from two points: the latest message we have access to or the first message from now on. That's a configuration that YOU must define
*		and add to the consumer properties.
*
*		The property who tells the consumer from which point it must start listening is the AUTO_OFFSET_RESET_CONFIG, that tells Kafka what to do when
*		there's not a initial offset in Kafka or if the current offset does not exist in the server anymore (because the data has been deleted). 
*
*			Let's imagine I'm starting a new consumer. There's no offset in it (he doesn't consumed any messages to have offsets inside of it), but
*			there are 5 messages in the broker. So... the consumer must start the offset from 0 or from 6? By default, Kafka make it start from last,
*			in other words, from 6. 
*
*				Starting from latest: 	you might lose messages if they already are in the broker but not in the consumer;
*
*				Starting from earliest:	imagine you receive messages, you process then... and your consumer falls. When it starts again, it might consume
*				the same messages once more (because they wasn't commited), and, therefore, execute the same job twice;
*
*			The AUTO_OFFSET_RESET_CONFIG has two values:
*
*				Smallest: automatically reset the offset to the smallest offset (if I have 1 million messages, but the oldest have been deleted and I 
*						  just have the latest 100.000, then the smallest will be the message 900.000, the smallest that Kafka is capable of deliver
*						  because it still on disk).
*						  
*				Latest:   the biggest of all messages that are stored in Kafka;
*
*					-> IMPORTANT: in the docs, Kafka says the property "latest" is actually "largest". That is wrong. There have been a change in it... and they
*						   	  	  didn't updated the docs.					
*
*				Disabled: throws exception to the consumer if no previous offset is found for the consumer's group. In other words: does not want to
*						  consume anything so it won't take the risk of consume a message again or lose messages;
*
*
*				In our code, we'll configure the AUTO_OFFSET_RESET_CONFIG's property to each one, so we can see how the consumer works with each one.
*		.   
*
*		IMPORTANT: this configurations will be applied only in the first time our consumer starts. If we run the consumer with a configuration and then try to
*				   change it, it will run with the same as before (unless we clean all data from our kafka from the folders we've defined inside the file
*				   server.properties)
*	
*
* 													CLASS 04_Dealing with duplicated messages
*
*	A. The duplicated message problem
*
*		The messages that a broker receives are committed from time to time. There's a default interval, but we can configure it ourselves. The thing is... sometimes
*		we want to make a commit manually. Imagine the situation: our consumer gets a message from the broker, process it and falls down. The broker was not updated
*		with the info that the message was consumed (it was not committed yet), so it still thinks the message needs to be consumed. A new consumer startup then, and
*		the message will be given to it... and the new consumer will process it again.
*
*		Stinks, doesn't?
*
*		So, how to deal with this? First, let's think how we can configure our services (consumers and producers) to work with the message sending:
*
*			1. We can configure the services to not care if we loose messages;
*
*			2. We can configure the services to make sure everybody must be in sync all the time (using "acks=all"), and then we can configure 
*			   the minimum of replicas that must be in sync
*
*				-> To configure the minimum number of replicas that must be in sync, we can use the property MIN_IN_SYNC_REPLICAS_CONFIG
*
*			3. We can configure the services to make sure only ONE message is received and processed;
*
*
*	B. Kafka Transaction
*
*		We started discussing the following POST: https://itnext.io/kafka-transaction-56f022af1b0c
*
*		So, there are situations where we want to consume a message at least once, sometimes we want to consume it at most once... but there are times we need it to be
*		consumed EXACTLY once. So we need to configure our Producer and our Consumer to works in this situation.
*
*		There are many configurations that we need to add to the Producer so it works like this (you can see then in the link above).
*
*		The consumer has much more work to do. We must:
*
*			1. Deactivate the auto commit (so noting will be committed while the consumer is processing the message);
*
*			2. Set the AUTO_OFFSET_RELEASE_CONFIG to earliest, so we can deal with all messages from the beginning;
*
*			3. Set the ISOLATION_LEVEL_CONFIG to read_committed, which means only committed (in the sense of "sended to the readers and all the necessary replicas")
*			   messages will be consumed;
*
*				-> There is other configurations to the isolation level. There are situations where when a message is written in the broker, some consumer might read
*				   it even if the producer did not received any confirmation, even with "ack=all".
*
*				-> So, as you can see, the "ack=all" and the "isolation level" can work together to make this work like a boss.
*
*
*		After this configurations, we must refactor our consumer service to work with some new code. In the link, they use a database to save the information about
*		the message that was processed. To do so, we must change our code to:
*
*			1. Add a new TransactionalConsumerRebalanceListener on subscribe, to help the consumer to rebalance the messages;
*
*			2. Ask the assignments (partitions) to our consumer, running through it all to access the informations of each one;
*
*			3. For each partition, we must:
*
*				3.1. Understand in which point we are inside the partition (what is the offset being processed). Since we've configured the consumer to read all
*					 messages from the beginning, we assume that we are storing the current messages in some place. In the link, the author assumes we are storing
*					 then in a database;
*				
*				3.2. Me make a "seek" in the consumer's partition, so it goes to the offset we've retrieved from the database. ;
*
*				3.3. Once we have the partition in the desired offset, we start consuming its messages (example: the last on in database is the fifth. So we start consuming the sixth);
*
*				3.4. We get the message, process it... then save in the database that the message X, from the group Y, topic Z and partition W, was saved.
*
*				3.5. After saving, we commit the database transaction and guarantee it will be saved only after the message was processed.
*
*
*		This approach will enable you us to process a message successfully one single time. If something goes wrong, you might want to process the message again, or 
*		at least try to, but if it WORKS... it will be processed only that time, never again. FUCK YEAH <3
*
*		But there is another approach... a more "natural" one. We will see it in the next class.
*
*
* 																CLASS 05_Idempotence
*
*	A. Natural ID and idempotence on the database
*
*		As you could see, the approach from last class works, but makes us have to work with the offsets ourselves. That's kind of a bummer, more so because Kafka already make it's 
*		magic with them.
*
*		But there is another way. A better way. The BIRD WAY! Sorry, I got carried away. Ok, so, about the better way: let's talk about the CreateUserService we have. It receives a
*		message, check if the user already exists in the database and then, ONLY THEN, saves it. That makes the collateral effects caused by a message received N not destructive, 
*		because it going to save it only one time and ignore all the others.
*
*			Idempotence: is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the 
*						 initial application. In other words: the situation that already happens in our CreateUserService when saving the same user twice.
*
*
*		Every message HAS an unique identifier. One possible way to identify it is like in the link from last class, using the "topic + consumer-group + partition + offset", since 
*		all messages has this informations - the problem in use them is that we have to make sure the rebalace is working properly, since the messages goes from one offset to another;
*
*			So... thinking about this, we might want to create OUR own identifier to the message, to use it without those problems.
*
*		So, let's start this class refactoring the CreateUserService to be executed in the threads way we've created.
*
*
*	B. Extracting the common database
*
*		After changing the last project, we start to look for others. The Fraud service, for instance; it will receive a message N times and check for frauds in it N times. In our little
*		app, thats not a problem; in real projects, it is - the fraud algorithm might not like this.
*
*		So, with that said, we must deal with this. To make it only work once, we'll execute as the CreateNewService, saving the messages in database so we can have a history if it was
*		already processed or not.
*
*		To do so, let's extract the database generation from CreateUserService to a common class, so all our services might use it if they wanted.
*
*	
*	C. Idempotence and Fast Delegate
*
*		Let's start updating the FraudService to use the database (remember, we want to make sure that a message will only be processed once, since there are times where our fraud's algorithm
*		can't take it being processed more than that). But, before that, let's update it to the new format, allowing it to execute different instances of the fraud service in different threads.
*
*		Once we've done that, we are going to create a frauds database to store the info about the messages, if they where already processed or not. We are going to create a table Orders that's
*		going to store the message id and if it was a fraud... and store that info. The problem: what should we use as the message id? We cannot generate the id inside our fraud service, because
*		if we do so the same message would have a different id generated to it each time it's consumed, therefore we would process it more than one time. Crapbaskets. So, what could we possibly
*		use as a message id?
*
*		If we review our own services, we can see that in the NewOrerServelet we create a orderId BEFORE send the message to kafka. We did that so we could use the Fast Delegate approach, and 
*		get rid of the message as soon as possible. So... we already have an orderId, that's generated before the message goes to kafka, so we do not have to worry with the message being processed
*		twice and adding a new orderId each time. In other words: the fast delegate approach saved us and helped us to have a unique ID for each of the messages that kafka receives and process 
*		inside of it.
*
*		Pretty dope, huh?
*
*
*	D. Idempotence in APIs
*
*		There are a lot of situations when we don't have an natural Id, like the one we saw in the topic C. For instance, if the user send an HTTP request with data from a buy, but that buy does
*		not keep state in the server (it's not part of a cart, it's just a request that shall be processed). In that schenario, we could receive an HTTP request with all the buy information and 
*		the client might press the F5 multiple times, therefore sending those info multiple times. In that aspect, the Fast Delegated we've used in last topic won't be enough, because each of 
*		those messages will reach the server with the SAME data, but different message's id (in other words: the request will be different, but the data being processed will be the same, so 
*		we'll keep processing something twice).
*
*		But, let's think: we are using a correlationId, remember? We say that every message in our system has an identification... that is the correlation Id (remember that happens because we
*		have an oneness between every correlationId inside our application. Let's suppose we are in one of the scenarios I've talked about, when the user can press F5 and send the same data in
*		a different HTTP... I'll always have an unique ID for each message, the correlationID. But what if the message was not send to the Messageria, but to the server, to my API, directly? 
*
*			In this situation, I'll have to a filter in my API, something to guarantee that the POST from the client has an unique id that will be received in every request with the same data,
*			and that identification shall be used to make the idempotence works inside my API (just like I created the correlationId on my NewOrderServelet, the client-side app should create
*			his id to send with the HTTP request).
*
*			But there's more: you'll agree we added an idempotence guarantee to the fraud-service. But what if we want to add it in other services? Should we just create databases inside
*			every service, one by one, to make sure this works? Well, no. In this kind of scenario, what makes more sense is create a layer responsible for defining if the message has already 
*			been executed or not.
*
*			So, we can do this. But, how? We could:
*
*				1. create a service specialized to do this. In this situation, the Fast Delegate would send a message to a topic a single service hears, and than
*				   it would guarantee the message is executed only once. 
*
*				2. add this validation to our Servlet. Think about it: if we just add this kind of validation in our servelet, besides guarantee all the messages where already processed 
*				   BEFORE they entered in Kafka's kingdom, we'll also make sure that any message that goes straight to my server and does not reach a Kafka's broker is unique.
*
*			The decision here depends more in how we want to distribute the responsibilities between our services than "whats right and whats wrong".
*
*			So:
*				
*					-> If the HTTP server (the API) can access the database, we could stick that validation there;
*					
*					-> If it's not, we could stick it into a service specialized in guarantee that the message is executed only once;
*
*							THIS is what we're going to do right now. 
*
*							OBS: in this scenario, it's important to keep the validation inside the fraud-service because it's something inherent to it's own functionalities.
*
*			Since we're going to validate this in the servlet, we're going to take some steps. First, we're going to create a connection to the database in each request (this is NOT a good
*			practice - the correct way would be create a connection poll from our servlet to consume from -, but we're doing it to make it simple and to keep with the class objectives).
*
*			