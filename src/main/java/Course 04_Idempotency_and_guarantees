*
* 												CLASS 01_Organization and e-mail services
* 
*	A. Reorganizing our project
*
*	 	Let's start this class refactoring our common-kafka, separating all the classes that works with the consumer and 
*		the ones that work with dispatchers in different packages.
* 
*		Then, let's talk about the e-mail service. Do we really need a service for that? Couldn't be just a library? Course
*		it could. The difficult whould be that the e-mail is an external system, and the way we'll communicate with it is 
*		important.
*
*
*	B. Discussing the e-mail service
*
*		Imagine that we've used the e-mail as a common library, as we're using the common-kafka. Now, I want to change the
*		way the e-mail is generated from using Azure to AWS. If the e-mail is just a library, every service that implements
*		it must migrate at same time BEFORE I can start use the new way, which creates a depedency between all the services
*		and the library.
*
*		That's a problem, a really bad one. Using a service, we can avoid that kind of problem, making the evolution of the
*		email something detached from the other services.
*
*		Still, there's something to consider: the way we're working right now, the e-mail text will be generated inside each
*		service that sends the message to the dispatcher. What if we wanted something greater? Or, thinking in another way:
*		what if we want to send an attachment in our e-mail? How could we deal with specific situations using something as
*		generic as the e-mail service?
*
*			Answer: we could create other, more specific services, to work as middle man in those situations and make the
*					specific configurations happen for us. Awesome, huh? The magic of microservices coming to life with
*					kafka.
*
*					To show this approach, we're going to create a little, tiny bit of program called service-email-new-order,
*					which will be responsible to create an e-mail that our service-new-order is currently creating by dispatching
*					the event in it's main.
*
*						Another advantage of this approach is that we're going to apply the Fast Delegate to the e-mail generation,
*						which is incredible!!!
*
*
* 														CLASS 02_The service layer
*
*
*	A. Extracting the KafkaService generation to a layer of services
*
*		Currently, if we want to run two instances at the same time, we need to run it manually. Each of the instances will be
*		run in it's own process. But there are situations where we might want to execute 10 e-mail services inside the same
*		process on own JVM (in other words, execute the instances in different threads).
*
*			This would make the startup's loading higher
*
*		So, in this class we going to start preparing the email service for this  First, but not least, we need to see if the
*		email has any state that it shares with it's methods. The problem is: it uses the KafkaService, that has a internal
*		state.
*
*		So, we need to see in the Kafka's docs if the Consumer and the Producer are thread safe. We can see that the producer
*		is... but the consume isn't. In its documentation, Kafka tells us how to deal with this situation and execute a consumer
*		in different threads. 
*
*			Well.. how are we going to deal with this?
*
*				In this project, we are not going to share the Kafka consumer in multi-threads... instead, we are going to create
*				one instance of service-email per thread (so each thread will got it's own consumer).
*
*
*		We are going to start by refactoring our email service, allowing it to be used as a service like it should be. You can see
*		in this comment how we are going to do it.
*
*
*	B. Parallelizing with threads's pools
*
*		Now that we've refactored our service-email, we can start to run it in multiple threads.
*
*		We have now the serviceProvider() function. We want it yo be called 10 times. One approach would be make it implement the runnable
*		interface. But we are not going to do this. The runnable does not allows to throw an exception inside of it... but there is another
*		one who does.
*
*		We want to call that function N times, so we make it implements a callable of type void (because it'll never return anything).
*
*		When you do this, you'll have N instances consuming in a service group, so its good to make sure that each one of then has it's 
*		own CLIENT_ID_CONFIG, a config we can add on the consumer properties (in our case, they already have).
*
*		But what if an exception is thrown when trying to generate a deadletter?
*
*			Then, the thread will die, but the others will keep running (actually, since we're creating with newFixedThreadPool, the library
*			will restart the thread that falls... BUT, our consumer won't be submited again to that thread. In other words: the thread will
*			be alive, our consumer won't).
*
*		Since we are here, let's go add the same "thread support" to the ReadingReportService. Once we've done it, we can run it and go to
*		the cmdline and see what are the consumer groups being executed. You'll see something like this, if you've been following this course
*		from the beginning:
*
* 			GROUP                                  					TOPIC                     PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  	LAG      									CONSUMER-ID 									HOST            		CLIENT-ID	
* 			                                          
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  1          0               0               0        7c1d3b0f-8d77-40af-8ab2-4f1aba6afd99-92d167c2-7fe9-4c9e-b190-9689d5440985 	/192.168.0.11   7c1d3b0f-8d77-40af-8ab2-4f1aba6afd99
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  2          0               0               0        830d4708-a65f-4da1-a090-df5989ec46cc-7190aacf-0192-49b8-9286-aff774f39055 	/192.168.0.11   830d4708-a65f-4da1-a090-df5989ec46cc
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  0          0               0               0        6fb31ff7-f7b9-444f-8793-012f323b7f63-d1145e5f-28dc-4bdf-9bfe-c8113d92a024 	/192.168.0.11   6fb31ff7-f7b9-444f-8793-012f323b7f63
* 			
* 			Consumer group 'EmailService' has no active members.
*
*
*		Note how there are three consumers being executed for the topic, the one we've changed. That happens because we've configure our kafka's 
* 		broker to use 3 partitions when executed... and, inside our code, we've chosen to run 5 threads with instances of the ReadingReportService.
*
*			In other words: there are 5 ReadingReportService instances running in 5 different threads, but just 3 of then are in kafka's 
*			partitions since we've enabled the broker to execute three partitions at time (remember, that was defined in the server.properties
*			file, that stays in config folder).
*
*
* 														CLASS 03_Commits and offsets
*
*
*	A. Offset latest and earliest
*
*		Remember: by default, our partitions use 3 replicas. Let's start this class erasing the data directory for each server and zookeper, and
*		restarting everything once more. Then, let's update our service-new-order to use the thread way we've created in last class, and put it 
*		to use a single thread.
*
*		Once done it, let's the HTTP service WITHOUT the service-new-email and see what happens. When we execute the /new of the http service, kafka 
*		will create a topic (the one called by the servlet), but no consumer group is created, since we've not started our consumer yet. Now, we run 
*		our service-new-order.
*
*		If you read the service-new-order logs, you'll discover it reset the partition offset. Why? When we start a new consumer group, it can begin
*		to listen from two points: the latest message we have access to or the first message from now on. That's a configuration that YOU must define
*		and add to the consumer properties.
*
*		The property who tells the consumer from which point it mus start listening is the AUTO_OFFSET_RESET_CONFIG, that tells Kafka what to do when
*		there's not a initial offset in Kafka or if the current offset does not exist in the server anymore (because the data has been deleted). 
*
*			Let's imagine I'm starting a new consumer. There's no offset in it (he doesn't consumed any messages to have offsets inside of it), but
*			there are 5 messages in the broker. So... the consumer must start the offset from 0 or from 6? By default, Kafka make it start from last,
*			in other words, from 6. 
*
*				Starting from latest: 	you might lose messages if they already are in the broker but not in the consumer;
*
*				Starting from earliest:	imagine you receive messages, you process then... and your consumer falls. When it starts again, it might consume
*				the same messages once more (because they wasn't commited), and, therefore, execute the same job twice;
*
*			The AUTO_OFFSET_RESET_CONFIG has two values:
*
*				Smallest: automatically reset the offset to the smallest offset (if I have 1 million messages, but the oldest have been deleted and I 
*						  just have the latest 100.000, then the smallest will be the message 900.000, the smallest that Kafka is capable of deliver
*						  because it still on disk).
*						  
*				Latest:   the biggest of all messages that are stored in Kafka;
*
*					-> IMPORTANT: in the docs, Kafka says the property "latest" is actually "largest". That is wrong. There have been a change in it... and they
*						   	  	  didn't updated the docs.					
*
*				Disabled: throws exception to the consumer if no previous offset is found for the consumer's group. In other words: does not want to
*						  consume anything so it won't take the risk of consume a message again or lose messages;
*
*
*				In our code, we'll configure the AUTO_OFFSET_RESET_CONFIG's property to each one, so we can see how the consumer works with each one.
*		.   
*
