*
* 											CLASS 01_New Producers and consumers
* 
* 
* 
* 		01. About changes in the service-fraud-detector
* 
* 			Let's start change the service and add a logic to understand if a value of a order is or not a fraud.
*  			Then, we are going to send messages from our service, cause we are baaaaaaad boys.
* 
* 			-> IMPORTANT: If we do execute the LogService BEFORE our the FraudService, who send messages to a new Topic, 
*			the LogService won't hear those messages. Why? Because when we use a pattern to hear the messages, the 
*			pattern will be applied ONLY when the service starts, but not later. In other words: since when it starts
*			we still doesn't have the FraudService with it's new topics running, it will not know those topics exists,
*			and, for that alone, it will not hear it.
*
*		02. About changes in Order of service-fraud-detector
*
*			Since we have a specific Order for each service, we make changes that only make sense inside that project. That
*			generates a trade-in-trade-off scenario: for a first, we don't have to add changes that does not make sense in
*			other scenarios. The problem is: we will have different models for each service. Do our project accepts that scenario?
*			Yes, so it's all sunny in Philly.
* 
* 		03. About the changes in KafkaService
*  
* 			Since we are now throwing exceptions in our consume methods (because we are going to send messages inside of the
* 			consume method from FraudService), we need to threat those exceptions. For now, we only will log in those exceptions.
* 
* 		04. About the new service CreateUserService
* 
* 			Our implementation will assume the service will run ONLY ONCE. This service will save the user information in a 
* 			SQLite database.
* 
* 			As we go further in the implementation of this service, one architectural decision is becoming clear: our user, when 
*			added, will not use the UniversalId as primary key, but the e-mail. When the person give their information, their
* 			identification will be the email, so this must be the key we will use to communicate between our services.
* 
* 			So, what does this mean? Since the Universal Id will exists only after the user is registered in my database, the
* 			key we must have in all the Order models is the email, and, as the project is when I'm writing this, the Order does
* 			not contain any email. With this in mind, we close this class and going to start the next one making this architectural
* 			change.
* 
* 
*  												CLASS 02_Envolving a service
* 
*  In this class, we discussed about evolving a service. We started to evolve to use email in Orders, and adding it to the
*  schemas of services that needed to know a order has an email (unfortunately, it was all the services). Besides that, we've
*  changed more bout the services and made them all work with our new model. To see more, look for the commits about this class.
* 
* 
*  													CLASS 03_HTTP Server
* 
*  Let's create a service, service-http-ecommerce, that will represent a webpage from where the user will send the first message
*  with it's data to the system. To do that, we'll use the HttpServlet library, and create a service which will start a server 
*  and create a message to send to our queue.
*  
*  FAST DELEGATE
*  
*  	Note that our newOrderServlet has little to no code before sending the message to Kafka. That happens because we are trying 
*  	to achieve a "fast delegate approach": we want to send the messages to the queue as fast as we can, so, if any error happens,
*  	we have our logs to show us where it did, and, even better, we only need to resend the message (if we did a lot of process
*  	before send it, and something went wrong, we'd have to redo all that process, besides send the message again. Not that, with
*  	approach, our work on errors will be reduced).
*  
*   With that in mind, our entry point will usually has little functionality in it, just to send the messages as soon as possible
*  	to not allow the risk that something goes wrong inside of it and we have to redo a process that the user already finished it.
*  
*  
*  													CLASS 04_Broker's clusters
*  
*  I. SINGLE POINT OF FAILURE DO BROKER
*  
*  	Now we have many services that we can let be running at same time. We can run any of these as many times as we want, and that's
*  	awesome. And, REMEMBER: each one of the consumer groups will consume, in parallel, at most, the number of partitions 
*  	that exists for their given topic.
*  
*  			EXAMPLE: 3 consumers will consume from 3 partitions, and Kafka will define which message goes for which one
*  
*  
*   THE PROBLEM: even if we can have many services running at same time, if the Kafka's broker goes down, it all goes down. Sucks, doesn't it?
*  
*  		GOOD TO KNOW: as we've used send().get(), the dispatcher will be waiting for the broker as much as it need to, so the program will get
*  					  stuck if we send a message with the broker down. Now, if the broker comes back, they will start working again and deliver
* 	 				  it the messages that got stuck, without any work from my part.
*  	
*  	To solve this problem, we MUST RUN MANY INSTANCES OF THE BROKER, as we do for the SERVICES :)
*  
*  
*  II. CLUSTER REPLICATION
*  
*  	We can reconfigure our broker while it's all running. Incredible, huh?
*  	
*  	To make that work, follow these steps:
*  	
*  		1. Copy the server.properties file to be used by the new broker;
*  
*  		2. Edit the new file to:
*  			2.1. Change the broker.id property;
*  			2.2. Change the log's directory to a new one, just for this broker;
*  			2.3. Change the listener's port to a new empty one;
*  
*  		3. Start a new instance with the new server.properties
*  	
*  	If we only do that and try to kill the first broke to see if Kafka will use the second, it does not work and the
*  	services will be waiting for the first one to come back.
*  	
*  	Why?
*  	
*  	Well, think about it: when we've created the consumer's partitions, they are all localized inside the first broker,
*  	and we didn't configure anything to tell then that, if that broker is down, they need to search for another. So, of
*  	course, they don't.
*  
*  	You can see this if you run the command:
*  	
*  						kafka-topics.bat --describe --bootstrap-server localhost:9092
*  
*  	It will show something like the following:
*  
* 		Topic: ECOMMERCE_ORDER_APPROVED PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
* 		        Topic: ECOMMERCE_ORDER_APPROVED Partition: 0    Leader: 0       Replicas: 0     Isr: 0
*
* 		Topic: ECOMMERCE_SEND_EMAIL     PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
* 		        Topic: ECOMMERCE_SEND_EMAIL     Partition: 0    Leader: 0       Replicas: 0     Isr: 0
*
* 		Topic: ECOMMERCE_ORDER_REJECTED PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
* 		        Topic: ECOMMERCE_ORDER_REJECTED Partition: 0    Leader: 0       Replicas: 0     Isr: 0
*
* 		Topic: ECOMMERCE_NEW_ORDER      PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
* 		        Topic: ECOMMERCE_NEW_ORDER      Partition: 0    Leader: 0       Replicas: 0     Isr: 0
*
* 		Topic: __consumer_offsets       PartitionCount: 50      ReplicationFactor: 1    Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600
*         Topic: __consumer_offsets       Partition: 0    Leader: 0       Replicas: 0     Isr: 0
*         Topic: __consumer_offsets       Partition: 1    Leader: 0       Replicas: 0     Isr: 0
*												...
*												...
*
*  	So, how do we deal with this?
*  	
*  	We need to change the ReplicationFactor of our topics. To do that, we can try to use the following command:
*  	
*  		kafka-topics.bat --zookeper localhost:2181 --alter --topic ECOMMERCE_NEW_ORDER --partitions 3 --replication-factor 2
*  	
*  	But that will give us an error. The replication factor CANNOT, and I quote, "CANNOT BE CHANGED AFTER THE TOPIC'S CREATION".
*  	
*  	So, what now? 
*  	
*  	Open the server.properties file from both brokers, and add the following line to then (in the example, they added it bellow the broker.id)
*  	
*  		default.replication.factor = 2;
*  	
*  	After that, stop the kafka and the zookeper, clean the log's directory folder aaaaand... lets give it a try!
*  	
*  		1. Restart Zookeper;
*  		2. Start kafka's first broker;
*  		3. Start kafka's second broker;
*  		4. Confirm there are no topics yet (since you deleted the log's directory, there should not have any)
*  		5. Run two instances of each service and let's get down to business;
*  	
*  	Open a new command line and use the command:
*  
*  						kafka-topics.bat --describe --bootstrap-server localhost:9092
*  	
*	It will generate something as bellow:
*
* 		Topic: ECOMMERCE_NEW_ORDER      PartitionCount: 3       ReplicationFactor: 2    Configs: segment.bytes=1073741824
* 		        Topic: ECOMMERCE_NEW_ORDER      Partition: 0    Leader: 2       Replicas: 2,0   Isr: 2,0
* 		        Topic: ECOMMERCE_NEW_ORDER      Partition: 1    Leader: 0       Replicas: 0,2   Isr: 0,2
* 		        Topic: ECOMMERCE_NEW_ORDER      Partition: 2    Leader: 2       Replicas: 2,0   Isr: 2,0
*
* 		Topic: ECOMMERCE_SEND_EMAIL     PartitionCount: 3       ReplicationFactor: 2    Configs: segment.bytes=1073741824
* 		        Topic: ECOMMERCE_SEND_EMAIL     Partition: 0    Leader: 2       Replicas: 2,0   Isr: 2,0
* 		        Topic: ECOMMERCE_SEND_EMAIL     Partition: 1    Leader: 0       Replicas: 0,2   Isr: 0,2
* 		        Topic: ECOMMERCE_SEND_EMAIL     Partition: 2    Leader: 2       Replicas: 2,0   Isr: 2,0
*
* 		Topic: __consumer_offsets       PartitionCount: 50      ReplicationFactor: 1    Configs: compression.type=producer,cleanup.policy=compact,segment.bytes=104857600
* 		        Topic: __consumer_offsets       Partition: 0    Leader: 2       Replicas: 2     Isr: 2
* 		        Topic: __consumer_offsets       Partition: 1    Leader: 0       Replicas: 0     Isr: 0
*												...
*												...
*
*			NOTE TO SELF: I've changed the "partitions" property to 3 when making the changes I'm talking here
*
*  	You can see by the table above all the topics created by your services, but, even better, the Leader property (that represents 
* 	the broker where the topic is running) will have other values than 0 (the 0 represented the id of the broker, so you'll see 
*	the number of the id you selected to the new broker here).
*  
*  	Now, when the Leader now falls Kafka will send the messages to any other Leaders available.
*  
*  	Note, thought, that there might be some topics where the "Leader" property is none for the topics "__consumer_offsets". Why? 
*  	Because the offsets, the messages I've already read, where all stored inside the broker that has fallen (if it comes back 
	again, then the process will resume from where it ended).
*  
*  		GOOD TO KNOW: The property ISR, that repeats bellow each of the topic's info when you run kafka-topics's command shows two
*  					  how many replics of that topic are updated until that moment.
*  
*  
*  III. A FIVE BROKER'S CLUSTER: EXPLORING LEADERS AND REPLICS
*  
*  	So, after seeing the problem with the "__consumer_offsets", we wonder what we could do, right? In the server.properties file, 
*  	there is a replication configuration specific to these topics, called "offsets.topic.replication.factor". You can see in the
*	the comments of that line that even the Kafka's team recommend to use a value different than one for environment differents  
*  	than development.
*  
*  	So, let's start this class with:
*  
*  		1. Change the line "offsets.topic.replication.factor" to 3;
*  		2. Change the line "transaction.state.log.replication.factor" to 3;
*  		3. Change the "default.replication.factor" to 3;
*  		4. Create the properties for two new clusters, and let's get these parties GOIIIING!!!
*  
*	  GOOD TO KNOW: if you do this process with your services running, when you start the brokers again the services will start on the brokers that 
*					already existed.
*	
*	In this class we've created more brokers and executed then, before starting to get brokers down and up to see how kafka react to each of the scenarios
*	
*	  GOOD TO KNOW: The ISR property can be seeing as "which brokers are updated with my message". You can get into scenarios
*					where the property "Replicas" says a topic is in the brokers "3,2,1", but IRS says the only ones that are up to date are "2,1".
*					That may happens when the broker 3 is down.
*
*	When a Leader is down, a Replica will try to assume it's place as a new Leader (but the Replica must have some properties that Kafka will look for)
*
*		When the Leader that went down comes back, the messages it looses will be sent again and he will be up to date with the others (but our services
*		not necessarily are going to run again, because they already ran in the Replicas that became leaders.
*
*
*
*	HAVE THIS IN MIND: when you create N brokers, you stop having 1 fail node and start to have N. What does that mean? It means that, if we have 3 nodes
*					   and they are all Replicas, and they all go down, then our Messageria will stop working, there is nothing we can do about it. Buuuut,
*					   the change all 3 are down is lower than the chance only one is down, so... yeah, it's better.
*
*
*  IV. ACKS AND RELIABILITY
*
*	What happens if we send a message, the message finds the Leader, but the Replicas are off? That's enough? Imagine we have the machines A, B, C, D. The first
*	three are off, and my message reach D, is not replicated and, before the other three can rise up again, D falls off too. Then, A, B and C gets up. What will
*	happen? One of the three will assume the Leader role, but the three will have the older information, and will not be updated with the message I've just sent, 
*	because there wasn't time to D replicate that the message.
*
*	So, how to deal with this? Have in mind that the .get() method we call after the send() method will wait for a confirmation from the Leader that everything
*	is OK. So, what we could do is make sure that get() will wait for a confirmation that the message sent to the Leader was delivery to each of the Replicas,
*	and that every Replica confirms it. 
*
*			-> This is the most reliable option, but it's also the slower approach. 
*
*			-> We can do that configuring the properties we pass to KafkaProducer in our file KafkaDispatcher.The property should be as follows:
*
*											properties.setProperty(ProducerConfig.ACKS_CONFIG, info_that_represents_the_config);
*
*				-> You can look the source code of ProducerConfig to see all options that ACKS_CONFIG can receive.
*
*				-> We have three types of acks:
*
*					1. "0", the producer will not wait for the Leader to respond;
*					2. "1", that makes the leader send the ack after write the message received in it's log;
*					3. "all", that we'll discuss bellow;
*
*			-> In our code, we will use the configuration "all", that means the Leader will await for the full confirmation from each of the Replicas that are 
*			   in sync (in the column IRS) before return a confirmation to the get() message.
*
*				-> The thing is: if I have N replicas, the only way to guarantee all of then will be updated before return a to the get() is using "all";
*
*				-> But then again, testing manually we saw some different behavior: when we run our program with this configuration, then stop the kafka's server
*				   from our replicas, the topics will be reorganized and Kafka will await for the ack from all the machines that still in sync... which will be 
*				   only the one we kept running.
*
*				-> So, if we have only one machine on it's foot, the ACKS_CONFIG doesn't do nothing. It will only make sense in situations where, if our service
*				   gets down, we try to raise them up in the next second... so there always will be more than one machine running in our cluster.
*
*
*		SO, TO HAVE THE MOST RELIABILITY POSSIBLE, WE NEED:
*
*			1. Many servers (replicas) running at the same time;
*			2. Many partitions running for a topic;
*			3. ACKS_CONFIG as "all", so we will keep synchronizing all of the replicas;
*
*