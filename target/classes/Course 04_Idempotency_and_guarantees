*
* 												CLASS 01_Organization and e-mail services
* 
*	A. Reorganizing our project
*
*	 	Let's start this class refactoring our common-kafka, separating all the classes that works with the consumer and 
*		the ones that work with dispatchers in different packages.
* 
*		Then, let's talk about the e-mail service. Do we really need a service for that? Couldn't be just a library? Course
*		it could. The difficult would be that the e-mail is an external system, and the way we'll communicate with it is 
*		important.
*
*
*	B. Discussing the e-mail service
*
*		Imagine that we've used the e-mail as a common library, as we're using the common-kafka. Now, I want to change the
*		way the e-mail is generated from using Azure to AWS. If the e-mail is just a library, every service that implements
*		it must migrate at same time BEFORE I can start use the new way, which creates a dependency between all the services
*		and the library.
*
*		That's a problem, a really bad one. Using a service, we can avoid that kind of problem, making the evolution of the
*		email something detached from the other services.
*
*		Still, there's something to consider: the way we're working right now, the e-mail text will be generated inside each
*		service that sends the message to the dispatcher. What if we wanted something greater? Or, thinking in another way:
*		what if we want to send an attachment in our e-mail? How could we deal with specific situations using something as
*		generic as the e-mail service?
*
*			Answer: we could create other, more specific services, to work as middle man in those situations and make the
*					specific configurations happen for us. Awesome, huh? The magic of microservices coming to life with
*					kafka.
*
*					To show this approach, we're going to create a little, tiny bit of program called service-email-new-order,
*					which will be responsible to create an e-mail that our service-new-order is currently creating by dispatching
*					the event in it's main.
*
*						Another advantage of this approach is that we're going to apply the Fast Delegate to the e-mail generation,
*						which is incredible!!!
*
*
* 														CLASS 02_The service layer
*
*
*	A. Extracting the KafkaService generation to a layer of services
*
*		Currently, if we want to run two instances at the same time, we need to run it manually. Each of the instances will be
*		run in it's own process. But there are situations where we might want to execute 10 e-mail services inside the same
*		process on own JVM (in other words, execute the instances in different threads).
*
*			This would make the startup's loading higher
*
*		So, in this class we going to start preparing the email service for this  First, but not least, we need to see if the
*		email has any state that it shares with it's methods. The problem is: it uses the KafkaService, that has a internal
*		state.
*
*		So, we need to see in the Kafka's docs if the Consumer and the Producer are thread safe. We can see that the producer
*		is... but the consume isn't. In its documentation, Kafka tells us how to deal with this situation and execute a consumer
*		in different threads. 
*
*			Well.. how are we going to deal with this?
*
*				In this project, we are not going to share the Kafka consumer in multi-threads... instead, we are going to create
*				one instance of service-email per thread (so each thread will got it's own consumer).
*
*
*		We are going to start by refactoring our email service, allowing it to be used as a service like it should be. You can see
*		in this comment how we are going to do it.
*
*
*	B. Parallelizing with threads's pools
*
*		Now that we've refactored our service-email, we can start to run it in multiple threads.
*
*		We have now the serviceProvider() function. We want it to be called 10 times. One approach would be make it implement the runnable
*		interface. But we are not going to do this. The runnable does not allows to throw an exception inside of it... but there is another
*		one who does.
*
*		We want to call that function N times, so we make it implements a callable of type void (because it'll never return anything).
*
*		When you do this, you'll have N instances consuming in a service group, so its good to make sure that each one of then has it's 
*		own CLIENT_ID_CONFIG, a config we can add on the consumer properties (in our case, they already have).
*
*		But what if an exception is thrown when trying to generate a deadletter?
*
*			Then, the thread will die, but the others will keep running (actually, since we're creating with newFixedThreadPool, the library
*			will restart the thread that falls... BUT, our consumer won't be submitted again to that thread. In other words: the thread will
*			be alive, our consumer won't).
*
*		Since we are here, let's go add the same "thread support" to the ReadingReportService. Once we've done it, we can run it and go to
*		the cmdline and see what are the consumer groups being executed. You'll see something like this, if you've been following this course
*		from the beginning:
*
* 			GROUP                                  					TOPIC                     PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  	LAG      									CONSUMER-ID 									HOST            		CLIENT-ID	
* 			                                          
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  1          0               0               0        7c1d3b0f-8d77-40af-8ab2-4f1aba6afd99-92d167c2-7fe9-4c9e-b190-9689d5440985 	/192.168.0.11   7c1d3b0f-8d77-40af-8ab2-4f1aba6afd99
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  2          0               0               0        830d4708-a65f-4da1-a090-df5989ec46cc-7190aacf-0192-49b8-9286-aff774f39055 	/192.168.0.11   830d4708-a65f-4da1-a090-df5989ec46cc
* 			ECOMMERCE_USER_GENERATE_READING_REPORT curso_kafka_ecommerce.ReadingReportService 	  0          0               0               0        6fb31ff7-f7b9-444f-8793-012f323b7f63-d1145e5f-28dc-4bdf-9bfe-c8113d92a024 	/192.168.0.11   6fb31ff7-f7b9-444f-8793-012f323b7f63
* 			
* 			Consumer group 'EmailService' has no active members.
*
*
*		Note how there are three consumers being executed for the topic, the one we've changed. That happens because we've configure our kafka's 
* 		broker to use 3 partitions when executed... and, inside our code, we've chosen to run 5 threads with instances of the ReadingReportService.
*
*			In other words: there are 5 ReadingReportService instances running in 5 different threads, but just 3 of then are in kafka's 
*			partitions since we've enabled the broker to execute three partitions at time (remember, that was defined in the server.properties
*			file, that stays in config folder).
*
*
* 														CLASS 03_Commits and offsets
*
*
*	A. Offset latest and earliest
*
*		Remember: by default, our partitions use 3 replicas. Let's start this class erasing the data directory for each server and zookeper, and
*		restarting everything once more. Then, let's update our service-new-order to use the thread way we've created in last class, and put it 
*		to use a single thread.
*
*		Once done it, let's the HTTP service WITHOUT the service-new-email and see what happens. When we execute the /new of the http service, kafka 
*		will create a topic (the one called by the servlet), but no consumer group is created, since we've not started our consumer yet. Now, we run 
*		our service-new-order.
*
*		If you read the service-new-order logs, you'll discover it reset the partition offset. Why? When we start a new consumer group, it can begin
*		to listen from two points: the latest message we have access to or the first message from now on. That's a configuration that YOU must define
*		and add to the consumer properties.
*
*		The property who tells the consumer from which point it must start listening is the AUTO_OFFSET_RESET_CONFIG, that tells Kafka what to do when
*		there's not a initial offset in Kafka or if the current offset does not exist in the server anymore (because the data has been deleted). 
*
*			Let's imagine I'm starting a new consumer. There's no offset in it (he doesn't consumed any messages to have offsets inside of it), but
*			there are 5 messages in the broker. So... the consumer must start the offset from 0 or from 6? By default, Kafka make it start from last,
*			in other words, from 6. 
*
*				Starting from latest: 	you might lose messages if they already are in the broker but not in the consumer;
*
*				Starting from earliest:	imagine you receive messages, you process then... and your consumer falls. When it starts again, it might consume
*				the same messages once more (because they wasn't commited), and, therefore, execute the same job twice;
*
*			The AUTO_OFFSET_RESET_CONFIG has two values:
*
*				Smallest: automatically reset the offset to the smallest offset (if I have 1 million messages, but the oldest have been deleted and I 
*						  just have the latest 100.000, then the smallest will be the message 900.000, the smallest that Kafka is capable of deliver
*						  because it still on disk).
*						  
*				Latest:   the biggest of all messages that are stored in Kafka;
*
*					-> IMPORTANT: in the docs, Kafka says the property "latest" is actually "largest". That is wrong. There have been a change in it... and they
*						   	  	  didn't updated the docs.					
*
*				Disabled: throws exception to the consumer if no previous offset is found for the consumer's group. In other words: does not want to
*						  consume anything so it won't take the risk of consume a message again or lose messages;
*
*
*				In our code, we'll configure the AUTO_OFFSET_RESET_CONFIG's property to each one, so we can see how the consumer works with each one.
*		.   
*
*	
*
* 													CLASS 04_Dealing with duplicated messages
*
*	A. The duplicated message problem
*
*		The messages that a broker receives are committed from time to time. There's a default interval, but we can configure it ourselves. The thing is... sometimes
*		we want to make a commit manually. Imagine the situation: our consumer gets a message from the broker, process it and falls down. The broker was not updated
*		with the info that the message was consumed (it was not committed yet), so it still thinks the message needs to be consumed. A new consumer startup then, and
*		the message will be given to it... and the new consumer will process it again.
*
*		Stinks, doesn't?
*
*		So, how to deal with this? First, let's think how we can configure our services (consumers and producers) to work with the message sending:
*
*			1. We can configure the services to not care if we loose messages;
*
*			2. We can configure the services to make sure everybody must be in sync all the time (using "acks=all"), and then we can configure 
*			   the minimum of replicas that must be in sync
*
*				-> To configure the minimum number of replicas that must be in sync, we can use the property MIN_IN_SYNC_REPLICAS_CONFIG
*
*			3. We can configure the services to make sure only ONE message is received and processed;
*
*
*	B. Kafka Transaction
*
*		We started discussing the following POST: https://itnext.io/kafka-transaction-56f022af1b0c
*
*		So, there are situations where we want to consume a message at least once, sometimes we want to consume it at most once... but there are times we need it to be
*		consumed EXACTLY once. So we need to configure our Producer and our Consumer to works in this situation.
*
*		There are many configurations that we need to add to the Producer so it works like this (you can see then in the link above).
*
*		The consumer has much more work to do. We must:
*
*			1. Deactivate the auto commit (so noting will be committed while the consumer is processing the message);
*
*			2. Set the AUTO_OFFSET_RELEASE_CONFIG to earliest, so we can deal with all messages from the beginning;
*
*			3. Set the ISOLATION_LEVEL_CONFIG to read_committed, which means only committed (in the sense of "sended to the readers and all the necessary replicas")
*			   messages will be consumed;
*
*				-> There is other configurations to the isolation level. There are situations where when a message is written in the broker, some consumer might read
*				   it even if the producer did not received any confirmation, even with "ack=all".
*
*				-> So, as you can see, the "ack=all" and the "isolation level" can work together to make this work like a boss.
*
*
*		After this configurations, we must refactor our consumer service to work with some new code. In the link, they use a database to save the information about
*		the message that was processed. To do so, we must change our code to:
*
*			1. Add a new TransactionalConsumerRebalanceListener on subscribe, to help the consumer to rebalance the messages;
*
*			2. Ask the assignments (partitions) to our consumer, running through it all to access the informations of each one;
*
*			3. For each partition, we must:
*
*				3.1. Understand in which point we are inside the partition (what is the offset being processed). Since we've configured the consumer to read all
*					 messages from the beginning, we assume that we are storing the current messages in some place. In the link, the author assumes we are storing
*					 then in a database;
*				
*				3.2. Me make a "seek" in the consumer's partition, so it goes to the offset we've retrieved from the database. ;
*
*				3.3. Once we have the partition in the desired offset, we start consuming its messages (example: the last on in database is the fifth. So we start consuming the sixth);
*
*				3.4. We get the message, process it... then save in the database that the message X, from the group Y, topic Z and partition W, was saved.
*
*				3.5. After saving, we commit the database transaction and guarantee it will be saved only after the message was processed.
*
*
*		This approach will enable you us to process a message successfully one single time. If something goes wrong, you might want to process the message again, or 
*		at least try to, but if it WORKS... it will be processed only that time, never again. FUCK YEAH <3
*
*		But there is another approach... a more "natural" one. We will see it in the next class.